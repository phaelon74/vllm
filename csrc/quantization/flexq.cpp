// SPDX-License-Identifier: Apache-2.0
// SPDX-FileCopyrightText: Copyright contributors to the vLLM project

#include <torch/extension.h>
#include <c10/cuda/CUDAGuard.h>
#include <c10/cuda/CUDAStream.h>

#include <vector>
#include <algorithm>
#include <cassert>

// Include FlexQ headers after standard library and torch headers
// to avoid conflicts with Python headers
#include "flexq/src/bgemm/flexq_bmma_library.h"
#include "flexq/src/bgemm/flexq_bmma_op.h"

#ifndef USE_ROCM

namespace vllm {
namespace flexq {

// Helper function to select appropriate kernel based on problem size
// Uses the function names generated by FQ_DECL_FUN/FQ_INSTANTIATE_FUN macros
// Function names are declared in flexq_bmma_library.h
// For cta<4,16,256> warp<32,48,128> mma<8,8,128> with 2 stages
// The macro FQ_DECL_FUN(FQBMMA, 8, 6, true, 4, 16, 256, 32, 48, 128, 8, 8, 128, 2, 1)
// generates: extern FQBMMAInitFn_t FQBMMA_8x6xtrue_4x16x256_32x48x128_8x8x128_2_1_InitFn;
// Since these are declared extern in the header, we can reference them directly
FQBMMAInitFn_t select_w6a8_kernel(int M, int N, int K) {
    // Select kernel based on M dimension
    // Using cta<4,16,256> warp<32,48,128> mma<8,8,128> with 2 stages as default
    // This kernel should handle most cases reasonably well
    // TODO: Implement proper kernel selection based on problem size
    // Use the function declared in flexq_bmma_library.h
    return FQBMMA_8x6xtrue_4x16x256_32x48x128_8x8x128_2_1_InitFn;
}

FQBMMAInitFn_t select_w6a16_kernel(int M, int N, int K) {
    // For W6A16, use same kernel selection as W6A8 for now
    // TODO: Adapt kernels specifically for A16 if needed
    return FQBMMA_8x6xtrue_4x16x256_32x48x128_8x8x128_2_1_InitFn;
}

FQBMMAExecFn_t get_exec_fn_w6a8(int M, int N, int K) {
    return FQBMMA_8x6xtrue_4x16x256_32x48x128_8x8x128_2_1_ExecFn;
}

FQBMMAExecFn_t get_exec_fn_w6a16(int M, int N, int K) {
    return FQBMMA_8x6xtrue_4x16x256_32x48x128_8x8x128_2_1_ExecFn;
}

// W6A8 GEMM kernel wrapper
torch::Tensor flexq_w6a8_gemm(
    torch::Tensor input,           // [M, K] int8 quantized activations
    torch::Tensor weight_packed,   // [N, K_packed] int8 packed weights
    torch::Tensor input_scale,      // [M, 1] or [1, 1] half scales for input
    torch::Tensor weight_scale,    // [N, group_size] or [N, 1] half scales for weights
    int group_size,
    bool bias = false) {
    
    TORCH_CHECK(input.is_cuda(), "input must be a CUDA tensor");
    TORCH_CHECK(weight_packed.is_cuda(), "weight_packed must be a CUDA tensor");
    TORCH_CHECK(input_scale.is_cuda(), "input_scale must be a CUDA tensor");
    TORCH_CHECK(weight_scale.is_cuda(), "weight_scale must be a CUDA tensor");
    
    TORCH_CHECK(input.dtype() == torch::kInt8, "input must be int8");
    TORCH_CHECK(weight_packed.dtype() == torch::kInt8, "weight_packed must be int8");
    TORCH_CHECK(input_scale.dtype() == torch::kFloat16 || input_scale.dtype() == torch::kBFloat16, 
                "input_scale must be half or bfloat16");
    TORCH_CHECK(weight_scale.dtype() == torch::kFloat16 || weight_scale.dtype() == torch::kBFloat16,
                "weight_scale must be half or bfloat16");
    
    const int M = input.size(0);
    const int K = input.size(1);
    const int N = weight_scale.size(0);
    
    // Ensure input_scale is half (float16)
    auto input_scale_half = input_scale.dtype() == torch::kFloat16 
        ? input_scale 
        : input_scale.to(torch::kFloat16);
    auto weight_scale_half = weight_scale.dtype() == torch::kFloat16
        ? weight_scale
        : weight_scale.to(torch::kFloat16);
    
    // Create output tensor
    auto output = torch::empty({M, N}, 
                               torch::TensorOptions()
                                   .dtype(torch::kFloat16)
                                   .device(input.device()));
    
    // Get CUDA stream
    at::cuda::CUDAStream stream = at::cuda::getCurrentCUDAStream(input.device().index());
    
    // Get pointers
    int* X_ptr = reinterpret_cast<int*>(input.data_ptr<int8_t>());
    int* W_ptr = reinterpret_cast<int*>(weight_packed.data_ptr<int8_t>());
    half* X_SCALE_ptr = reinterpret_cast<half*>(input_scale_half.data_ptr<at::Half>());
    half* W_SCALE_ptr = reinterpret_cast<half*>(weight_scale_half.data_ptr<at::Half>());
    half* D_ptr = reinterpret_cast<half*>(output.data_ptr<at::Half>());
    
    // Select kernel
    FQBMMAInitFn_t init_fn = select_w6a8_kernel(M, N, K);
    FQBMMAExecFn_t exec_fn = get_exec_fn_w6a8(M, N, K);
    
    // Initialize kernel state
    FQBMMAOpState state = init_fn(X_ptr, W_ptr, X_SCALE_ptr, W_SCALE_ptr, 
                                  M, N, K, D_ptr, group_size, bias);
    
    if (!state.initSuccess) {
        TORCH_CHECK(false, "FlexQ kernel initialization failed");
    }
    
    // Execute kernel
    exec_fn(state, stream.stream());
    
    // Check for errors
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        TORCH_CHECK(false, "FlexQ kernel execution failed: ", cudaGetErrorString(err));
    }
    
    return output;
}

// W6A16 GEMM kernel wrapper
torch::Tensor flexq_w6a16_gemm(
    torch::Tensor input,           // [M, K] float16 activations (will be quantized to int8 internally)
    torch::Tensor weight_packed,   // [N, K_packed] int8 packed weights
    torch::Tensor input_scale,     // [M, 1] or [1, 1] half scales for input
    torch::Tensor weight_scale,    // [N, group_size] or [N, 1] half scales for weights
    int group_size,
    bool bias = false) {
    
    TORCH_CHECK(input.is_cuda(), "input must be a CUDA tensor");
    TORCH_CHECK(weight_packed.is_cuda(), "weight_packed must be a CUDA tensor");
    TORCH_CHECK(input_scale.is_cuda(), "input_scale must be a CUDA tensor");
    TORCH_CHECK(weight_scale.is_cuda(), "weight_scale must be a CUDA tensor");
    
    TORCH_CHECK(input.dtype() == torch::kFloat16 || input.dtype() == torch::kBFloat16,
                "input must be float16 or bfloat16");
    TORCH_CHECK(weight_packed.dtype() == torch::kInt8, "weight_packed must be int8");
    TORCH_CHECK(input_scale.dtype() == torch::kFloat16 || input_scale.dtype() == torch::kBFloat16,
                "input_scale must be half or bfloat16");
    TORCH_CHECK(weight_scale.dtype() == torch::kFloat16 || weight_scale.dtype() == torch::kBFloat16,
                "weight_scale must be half or bfloat16");
    
    const int M = input.size(0);
    const int K = input.size(1);
    const int N = weight_scale.size(0);
    
    // Convert input to float16 if needed
    auto input_half = input.dtype() == torch::kFloat16 
        ? input 
        : input.to(torch::kFloat16);
    auto input_scale_half = input_scale.dtype() == torch::kFloat16 
        ? input_scale 
        : input_scale.to(torch::kFloat16);
    auto weight_scale_half = weight_scale.dtype() == torch::kFloat16
        ? weight_scale
        : weight_scale.to(torch::kFloat16);
    
    // For W6A16, we need to quantize the FP16 activations to int8 first
    // The FlexQ kernel expects int8 quantized activations
    // Quantize on GPU: input_int8 = round(input_fp16 / scale)
    // Handle per-token or per-tensor scaling
    auto input_quantized = torch::empty({M, K}, 
                                        torch::TensorOptions()
                                            .dtype(torch::kInt8)
                                            .device(input.device()));
    
    // Quantize using torch operations (GPU-accelerated)
    if (input_scale_half.numel() == 1) {
        // Per-tensor quantization
        auto scale_val = input_scale_half.item<at::Half>();
        input_quantized = (input_half / scale_val).round().clamp(-128, 127).to(torch::kInt8);
    } else {
        // Per-token quantization
        auto scale_expanded = input_scale_half.expand({M, 1});
        input_quantized = (input_half / scale_expanded).round().clamp(-128, 127).to(torch::kInt8);
    }
    
    // Create output tensor
    auto output = torch::empty({M, N}, 
                               torch::TensorOptions()
                                   .dtype(torch::kFloat16)
                                   .device(input.device()));
    
    // Get CUDA stream
    at::cuda::CUDAStream stream = at::cuda::getCurrentCUDAStream(input.device().index());
    
    // Get pointers
    int* X_ptr = reinterpret_cast<int*>(input_quantized.data_ptr<int8_t>());
    int* W_ptr = reinterpret_cast<int*>(weight_packed.data_ptr<int8_t>());
    half* X_SCALE_ptr = reinterpret_cast<half*>(input_scale_half.data_ptr<at::Half>());
    half* W_SCALE_ptr = reinterpret_cast<half*>(weight_scale_half.data_ptr<at::Half>());
    half* D_ptr = reinterpret_cast<half*>(output.data_ptr<at::Half>());
    
    // Select kernel (using W6A8 kernel for now, should be adapted for A16)
    FQBMMAInitFn_t init_fn = select_w6a16_kernel(M, N, K);
    FQBMMAExecFn_t exec_fn = get_exec_fn_w6a16(M, N, K);
    
    // Initialize kernel state
    FQBMMAOpState state = init_fn(X_ptr, W_ptr, X_SCALE_ptr, W_SCALE_ptr, 
                                  M, N, K, D_ptr, group_size, bias);
    
    if (!state.initSuccess) {
        TORCH_CHECK(false, "FlexQ kernel initialization failed");
    }
    
    // Execute kernel
    exec_fn(state, stream.stream());
    
    // Check for errors
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        TORCH_CHECK(false, "FlexQ kernel execution failed: ", cudaGetErrorString(err));
    }
    
    return output;
}

} // namespace flexq
} // namespace vllm

#endif // USE_ROCM

