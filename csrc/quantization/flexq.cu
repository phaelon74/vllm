// SPDX-License-Identifier: Apache-2.0
// SPDX-FileCopyrightText: Copyright contributors to the vLLM project

// Explicitly include Python.h FIRST to ensure Python headers are fully initialized
// This must come before any standard library headers that might conflict
#define PY_SSIZE_T_CLEAN
#include <Python.h>

// Now include PyTorch headers (torch/extension.h expects Python.h to be included)
#include <torch/extension.h>
#include <c10/cuda/CUDAGuard.h>
#include <c10/cuda/CUDAStream.h>

// Include standard library headers
#include <vector>
#include <algorithm>
#include <cassert>

// Include FlexQ headers last - they include base.h and flexq_bmma_op.h (which has iostream)
// Python headers are now fully initialized, so iostream shouldn't conflict
#include "flexq/src/bgemm/flexq_bmma_library.h"
#include "flexq/src/bgemm/flexq_bmma_op.h"

#ifndef USE_ROCM

namespace vllm {
namespace flexq {

// Helper function to select appropriate kernel based on problem size
// Uses the function names generated by FQ_DECL_FUN/FQ_INSTANTIATE_FUN macros
// Function names are declared in flexq_bmma_library.h
// For cta<4,16,256> warp<32,48,128> mma<8,8,128> with 2 stages
// The macro FQ_DECL_FUN(FQBMMA, 8, 6, true, 4, 16, 256, 32, 48, 128, 8, 8, 128, 2, 1)
// generates: extern FQBMMAInitFn_t FQBMMA_8x6xtrue_4x16x256_32x48x128_8x8x128_2_1_InitFn;
// Since these are declared extern in the header, we can reference them directly
FQBMMAInitFn_t select_w6a8_kernel(int M, int N, int K) {
    // Select kernel based on M dimension
    // Using cta<4,16,256> warp<32,48,128> mma<8,8,128> with 2 stages as default
    // This kernel should handle most cases reasonably well
    // TODO: Implement proper kernel selection based on problem dimensions
    return FQBMMA_8x6xtrue_4x16x256_32x48x128_8x8x128_2_1_InitFn;
}

FQBMMAInitFn_t select_w6a16_kernel(int M, int N, int K) {
    // For W6A16, we'll use the same W6A8 kernels but quantize activations to int8 first
    // TODO: Implement proper kernel selection
    return FQBMMA_8x6xtrue_4x16x256_32x48x128_8x8x128_2_1_InitFn;
}

} // namespace flexq
} // namespace vllm

// W6A8 GEMM function
torch::Tensor flexq_w6a8_gemm(
    torch::Tensor x,           // Input activations (FP16)
    torch::Tensor w,           // Packed weights (int8/int32)
    torch::Tensor x_scale,     // Activation scales (FP16)
    torch::Tensor w_scale,     // Weight scales (FP16)
    int group_size) {
    
    TORCH_CHECK(x.dtype() == torch::kFloat16, "x must be FP16");
    TORCH_CHECK(w.dtype() == torch::kInt8 || w.dtype() == torch::kInt32, "w must be int8 or int32");
    TORCH_CHECK(x_scale.dtype() == torch::kFloat16, "x_scale must be FP16");
    TORCH_CHECK(w_scale.dtype() == torch::kFloat16, "w_scale must be FP16");
    
    int M = x.size(0);
    int K = x.size(1);
    int N = w.size(0);
    
    // Create output tensor
    auto options = torch::TensorOptions().dtype(torch::kFloat16).device(x.device());
    auto output = torch::empty({M, N}, options);
    
    // Get CUDA stream
    at::cuda::CUDAStream stream = at::cuda::getCurrentCUDAStream();
    
    // Select kernel
    vllm::flexq::FQBMMAInitFn_t init_fn = vllm::flexq::select_w6a8_kernel(M, N, K);
    
    // Prepare inputs
    // Note: FlexQ kernels expect quantized activations (int8), but we're passing FP16
    // We need to quantize activations to int8 first
    // For now, this is a placeholder - actual quantization should be done here
    
    // Convert tensors to the expected types
    auto x_int8 = x.to(torch::kInt8);  // This is a placeholder - actual quantization needed
    auto w_int32 = w.to(torch::kInt32);  // Unpack if needed
    
    // Call the kernel initialization function
    FQBMMAOpState state = init_fn(
        x_int8.data_ptr<int>(),
        w_int32.data_ptr<int>(),
        x_scale.data_ptr<at::Half>(),
        w_scale.data_ptr<at::Half>(),
        M, N, K,
        output.data_ptr<at::Half>(),
        group_size,
        false  // bias
    );
    
    if (!state.initSuccess) {
        TORCH_CHECK(false, "FlexQ kernel initialization failed");
    }
    
    // Execute the kernel
    vllm::flexq::FQBMMAExecFn_t exec_fn = FQBMMA_8x6xtrue_4x16x256_32x48x128_8x8x128_2_1_ExecFn;
    exec_fn(state, stream.stream());
    
    return output;
}

// W6A16 GEMM function
torch::Tensor flexq_w6a16_gemm(
    torch::Tensor x,           // Input activations (FP16)
    torch::Tensor w,           // Packed weights (int8/int32)
    torch::Tensor x_scale,     // Activation scales (FP16)
    torch::Tensor w_scale,     // Weight scales (FP16)
    int group_size) {
    
    TORCH_CHECK(x.dtype() == torch::kFloat16, "x must be FP16");
    TORCH_CHECK(w.dtype() == torch::kInt8 || w.dtype() == torch::kInt32, "w must be int8 or int32");
    TORCH_CHECK(x_scale.dtype() == torch::kFloat16, "x_scale must be FP16");
    TORCH_CHECK(w_scale.dtype() == torch::kFloat16, "w_scale must be FP16");
    
    // For W6A16, we quantize FP16 activations to int8 and use W6A8 kernels
    // This is a simplified approach - ideally we'd have dedicated W6A16 kernels
    
    // Quantize activations from FP16 to int8
    // TODO: Implement proper quantization
    auto x_int8 = x.to(torch::kInt8);  // Placeholder
    
    // Use the W6A8 kernel
    return flexq_w6a8_gemm(x_int8, w, x_scale, w_scale, group_size);
}

#endif // USE_ROCM

