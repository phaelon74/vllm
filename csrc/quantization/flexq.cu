// SPDX-License-Identifier: Apache-2.0
// SPDX-FileCopyrightText: Copyright contributors to the vLLM project

// Use torch/all.h like other CUDA files (e.g., q_gemm.cu)
// This provides torch::Tensor without Python header conflicts in CUDA context
#include <torch/all.h>
#include <c10/cuda/CUDAGuard.h>
#include <c10/cuda/CUDAStream.h>

#include <vector>
#include <algorithm>
#include <cassert>

// Include FlexQ headers - these contain CUDA device code
#include "flexq/src/bgemm/flexq_bmma_library.h"
#include "flexq/src/bgemm/flexq_bmma_op.h"

#ifndef USE_ROCM

namespace vllm {
namespace flexq {

// Helper function to select appropriate kernel based on problem size
// Uses the function names generated by FQ_DECL_FUN/FQ_INSTANTIATE_FUN macros
// Function names are declared in flexq_bmma_library.h
// For cta<4,16,256> warp<32,48,128> mma<8,8,128> with 2 stages
// The macro FQ_DECL_FUN(FQBMMA, 8, 6, true, 4, 16, 256, 32, 48, 128, 8, 8, 128, 2, 1)
// generates: extern FQBMMAInitFn_t FQBMMA_8x6xtrue_4x16x256_32x48x128_8x8x128_2_1_InitFn;
// Since these are declared extern in the header, we can reference them directly
// Note: FQBMMAInitFn_t and FQBMMAExecFn_t are defined in global namespace, not vllm::flexq
FQBMMAInitFn_t select_w6a8_kernel(int M, int N, int K) {
    // Select kernel based on M dimension
    // Using cta<4,16,256> warp<32,48,128> mma<8,8,128> with 2 stages as default
    // This kernel should handle most cases reasonably well
    // TODO: Implement proper kernel selection based on problem dimensions
    return FQBMMA_8x6xtrue_4x16x256_32x48x128_8x8x128_2_1_InitFn;
}

FQBMMAInitFn_t select_w6a16_kernel(int M, int N, int K) {
    // For W6A16, we'll use the same W6A8 kernels but quantize activations to int8 first
    // TODO: Implement proper kernel selection
    return FQBMMA_8x6xtrue_4x16x256_32x48x128_8x8x128_2_1_InitFn;
}

} // namespace flexq
} // namespace vllm

// W6A8 GEMM function
torch::Tensor flexq_w6a8_gemm(
    torch::Tensor input,           // Input activations (FP16)
    torch::Tensor weight_packed,   // Packed weights (int8/int32)
    torch::Tensor input_scale,     // Activation scales (FP16)
    torch::Tensor weight_scale,    // Weight scales (FP16)
    int64_t group_size,
    bool bias) {
    
    TORCH_CHECK(input.dtype() == torch::kFloat16, "input must be FP16");
    TORCH_CHECK(weight_packed.dtype() == torch::kInt8 || weight_packed.dtype() == torch::kInt32, "weight_packed must be int8 or int32");
    TORCH_CHECK(input_scale.dtype() == torch::kFloat16, "input_scale must be FP16");
    TORCH_CHECK(weight_scale.dtype() == torch::kFloat16, "weight_scale must be FP16");
    
    // Convert weight_packed to int32 unconditionally (FlexQ kernels require int32)
    // This must happen before any other operations to ensure PyTorch compiler sees it
    // Always convert to ensure dtype is int32, even if it's already int32
    // This prevents PyTorch compiler from optimizing away the conversion
    // Use .contiguous() to ensure the tensor is properly formatted
    torch::Tensor weight_int32 = weight_packed.to(torch::kInt32).contiguous();
    
    int M = input.size(0);
    int K = input.size(1);
    int N = weight_int32.size(0);
    
    // Create output tensor
    auto options = torch::TensorOptions().dtype(torch::kFloat16).device(input.device());
    auto output = torch::empty({M, N}, options);
    
    // Get CUDA stream
    at::cuda::CUDAStream stream = at::cuda::getCurrentCUDAStream();
    
    // Select kernel (FQBMMAInitFn_t is in global namespace)
    FQBMMAInitFn_t init_fn = vllm::flexq::select_w6a8_kernel(M, N, K);
    
    // Prepare inputs
    // FlexQ kernels expect int* for both X and W inputs
    // Convert input to int32 (quantization will be handled by the kernel or preprocessing)
    // For now, we convert FP16 to int32 as a placeholder
    // TODO: Implement proper quantization from FP16 to int8/int32
    torch::Tensor input_int32 = input.to(torch::kInt32).contiguous();
    
    // Call the kernel initialization function
    // Note: FlexQ uses CUDA's half type (__half), not at::Half
    // We need to cast the pointers appropriately
    FQBMMAOpState state = init_fn(
        input_int32.data_ptr<int>(),
        weight_int32.data_ptr<int>(),
        reinterpret_cast<half*>(input_scale.data_ptr<at::Half>()),
        reinterpret_cast<half*>(weight_scale.data_ptr<at::Half>()),
        M, N, K,
        reinterpret_cast<half*>(output.data_ptr<at::Half>()),
        static_cast<int>(group_size),
        bias
    );
    
    if (!state.initSuccess) {
        TORCH_CHECK(false, "FlexQ kernel initialization failed");
    }
    
    // Execute the kernel (FQBMMAExecFn_t is in global namespace)
    FQBMMAExecFn_t exec_fn = FQBMMA_8x6xtrue_4x16x256_32x48x128_8x8x128_2_1_ExecFn;
    exec_fn(state, stream.stream());
    
    return output;
}

// W6A16 GEMM function
torch::Tensor flexq_w6a16_gemm(
    torch::Tensor input,           // Input activations (FP16)
    torch::Tensor weight_packed,   // Packed weights (int8/int32)
    torch::Tensor input_scale,     // Activation scales (FP16)
    torch::Tensor weight_scale,    // Weight scales (FP16)
    int64_t group_size,
    bool bias) {
    
    TORCH_CHECK(input.dtype() == torch::kFloat16, "input must be FP16");
    TORCH_CHECK(weight_packed.dtype() == torch::kInt8 || weight_packed.dtype() == torch::kInt32, "weight_packed must be int8 or int32");
    TORCH_CHECK(input_scale.dtype() == torch::kFloat16, "input_scale must be FP16");
    TORCH_CHECK(weight_scale.dtype() == torch::kFloat16, "weight_scale must be FP16");
    
    // For W6A16, we quantize FP16 activations to int8 and use W6A8 kernels
    // This is a simplified approach - ideally we'd have dedicated W6A16 kernels
    
    // Quantize activations from FP16 to int8
    // TODO: Implement proper quantization
    auto input_int8 = input.to(torch::kInt8);  // Placeholder
    
    // Use the W6A8 kernel
    return flexq_w6a8_gemm(input_int8, weight_packed, input_scale, weight_scale, group_size, bias);
}

#endif // USE_ROCM

